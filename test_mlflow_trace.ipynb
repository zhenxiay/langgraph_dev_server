{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f1ed45b-0695-4861-bf45-90bcf7a44808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Enviroment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cefdb6b6-0827-4fac-9692-5fc931f9a7e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297b4046-ccbe-4a40-9324-8daab6942083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(scope = \"langgraph\", key = \"openai-api-key\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = dbutils.secrets.get(scope = \"langgraph\", key = \"anthropic-api-key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aea82bc-3d10-4d75-8c23-8dd49b608f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define in memory store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b79581d6-311e-4c1a-a9dd-dbc1df907720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "in_memory_embd_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\n",
    "        \"dims\": 1536,                              # Embedding dimensions\n",
    "        \"fields\": [\"content\"]              # Fields to embed - updated to 'content'\n",
    "    }\n",
    ")\n",
    "\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"embd_memory_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa3e6fab-7687-420f-823f-82d02bfcdcfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define tools and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f64086dd-13bd-46ad-9715-3b212501fc7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "import requests\n",
    "\n",
    "# init LLM\n",
    "llm = init_chat_model(\n",
    "    \"openai:gpt-4o\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def get_weather(latitude, longitude):\n",
    "    \"\"\"Get the weather for a given latitude and longitude\"\"\"\n",
    "\n",
    "    base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "    other_params = \"current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n",
    "\n",
    "    response = requests.get(\n",
    "        f\"{base_url}?latitude={latitude}&longitude={longitude}&{other_params}\"\n",
    "    )\n",
    "    data = response.json()\n",
    "    return data[\"current\"][\"temperature_2m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7baa5863-be53-4f61-9a90-c4ec84a4e555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6694bac-8808-42b4-935e-6729fe68e122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"get_news\": {\n",
    "            # Make sure you start your weather server on port 8000\n",
    "            \"url\": \"https://mcp-server-sse-91511769710.europe-west3.run.app/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "mcp_tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e7e436-9f21-4867-a2f3-beac54145f23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for tool in mcp_tools:\n",
    "    print(tool.get_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375c50af-440d-40bc-ae7b-c1e0f9ae3372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Augment the LLM with tools\n",
    "tools = [get_weather] + [tool for tool in mcp_tools]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40f10c7-e39f-4eb3-af1c-806aa5ba2501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40e8c4ec-8d42-4a1e-9cc8-7758df640ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define state structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef0ecaf-998c-4bed-9212-fffd6a84b9b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import AnyMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    llm_calls: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0024edbe-6543-4969-9388-dbe3c588f1d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define model node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9691c3cd-1fb2-44b1-ae85-09081535d8fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages.utils import (\n",
    "    trim_messages,\n",
    "    count_tokens_approximately\n",
    ")\n",
    "\n",
    "def message_trimmer(state: MessagesState):\n",
    "    messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=256,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1b37725-6d8e-4979-9738-814198ce50d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "def llm_call(state: dict):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        #content=system_prompt_with_rag(state)\n",
    "                        content='You are a helpful agent! Use the available tools to find the answer!'\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ],\n",
    "        \"llm_calls\": state.get('llm_calls', 0) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa68559b-f8a0-448c-bb58-54f86168f5e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define tool node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b063cc-1b9a-4969-aab5-ad95fe8fd695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage, AIMessage, HumanMessage\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e198b0e-a262-4977-a357-16815ffaa15a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Add shoud countinue logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be80516a-095e-4cd0-9bad-8a8d1ba7915a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: MessagesState) -> Literal[\"tool_node\", \"update_memory\"]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return \"update_memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7391f06a-0b40-47ee-bac4-bdf3a8aef83a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Build and show agent graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258a5e2c-f304-4608-aff6-33012d4809a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_memory(state: dict):\n",
    "    memory_id = str(uuid.uuid4())\n",
    "    # Extract content from messages and store as a string\n",
    "    memory_content = \"\"\n",
    "    for m in state['messages']:\n",
    "        if isinstance(m, HumanMessage):\n",
    "            memory_content += \"Human: \" + str(m.content) + \" \"\n",
    "        elif isinstance(m, AIMessage):\n",
    "            memory_content += \"AI: \" + str(m.content) + \" \"\n",
    "    memory_content = memory_content.strip()\n",
    "\n",
    "    memory = {\"content\" : memory_content}  # Store the content in a serializable format\n",
    "    in_memory_embd_store.put(namespace_for_memory,\n",
    "                            memory_id,\n",
    "                            memory,\n",
    "                            index=['content']) # Index on the new 'content' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7f9b33-af79-43d8-97da-6376c6790a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "saver = InMemorySaver()\n",
    "\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "#agent_builder.add_node(\"rag\", rag)\n",
    "#agent_builder.add_node(\"search_memory\", search_memory)\n",
    "agent_builder.add_node(\"pre_model_hook\", message_trimmer)\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"update_memory\", update_memory)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "#agent_builder.add_edge(START, \"rag\")\n",
    "#agent_builder.add_edge(\"rag\", \"llm_call\")\n",
    "#agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_edge(START, \"pre_model_hook\")\n",
    "#agent_builder.add_edge(START,\"search_memory\")\n",
    "#agent_builder.add_edge(\"search_memory\", \"pre_model_hook\")\n",
    "agent_builder.add_edge(\"pre_model_hook\", \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    #[\"tool_node\", END]\n",
    "    [\"tool_node\", \"update_memory\"]\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "#agent_builder.add_edge(\"llm_call\", \"update_memory\")\n",
    "agent_builder.add_edge(\"update_memory\", END)\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile(checkpointer=saver,\n",
    "                              store=in_memory_embd_store)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "# Show the agent\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b257452b-bd94-4dc9-8a03-60e4ccced8d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Set up mlflow experiment to trace agent workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53123af1-e39b-411b-a0ea-9cbfb04682e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.login()\n",
    "mlflow.set_experiment('/Agent-Workflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88011c2a-6484-4703-ac54-55f13cdf4388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a332a2d-bb88-47b5-91d3-7b5e91b3f698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create thread and run agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0752e6-8743-4d76-b038-8ba39faa8a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure thread\n",
    "import uuid\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "input_message = input('Input your question for the agent: ')\n",
    "\n",
    "# Invoke\n",
    "from langchain_core.messages import HumanMessage, BaseMessage # Import BaseMessage for type checking\n",
    "messages = [HumanMessage(content=input_message)]\n",
    "messages = await agent.ainvoke({\"messages\": messages}, config)\n",
    "#for m in messages[\"messages\"]:\n",
    "#    if isinstance(m, BaseMessage): # Check if it's a message object\n",
    "#        m.pretty_print()\n",
    "#    else:\n",
    "#        print(m) # Print the string directly if not a message object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c2ef7a-f1f4-4a5e-8ea6-f0ad6a87b080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.search_traces().head(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/yu.zhenxiao.yz@gmail.com/langgraph_dev_server/requirements.txt",
     "langchain",
     "langchain_mcp_adapters"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_mlflow_trace",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
